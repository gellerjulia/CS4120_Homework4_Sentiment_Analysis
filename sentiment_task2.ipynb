{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: Julia Geller (4120) and Shae Marks (4120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Train a Naive Bayes Model (30 points)\n",
    "----\n",
    "\n",
    "Using `nltk`'s `NaiveBayesClassifier` class, train a Naive Bayes classifier using a Bag of Words as features.\n",
    "https://www.nltk.org/_modules/nltk/classify/naivebayes.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils\n",
    "\n",
    "# nltk for Naive Bayes and metrics\n",
    "import nltk\n",
    "import nltk.classify.util\n",
    "from nltk.metrics.scores import (precision, recall, f_measure, accuracy)\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# some potentially helpful data structures from collections\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# so that we can make plots\n",
    "import matplotlib.pyplot as plt\n",
    "# if you want to use seaborn to make plots\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define constants for the files we are using\n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First part of tuple is an array of tokenized reviews (represented as arrays of tokens)\n",
      "Second part of tuple is an array of labels for the reviews\n",
      "['Van', 'Dien', 'must', 'cringe', 'with', 'embarrassment', 'at', 'the', 'memory', 'of'] 0\n"
     ]
    }
   ],
   "source": [
    "# load in your data and make sure you understand the format\n",
    "# Do not print out too much so as to impede readability of your notebook\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "X_train, y_train = train_tups\n",
    "X_dev, y_dev = dev_tups\n",
    "\n",
    "print(\"First part of tuple is an array of tokenized reviews (represented as arrays of tokens)\")\n",
    "print(\"Second part of tuple is an array of labels for the reviews\")\n",
    "print(X_train[0][:10], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a sentiment classifier using NLTK's NaiveBayesClassifier and \n",
    "# a bag of words as features\n",
    "# take a look at the function in lecture notebook 7 (feel free to copy + paste that function)\n",
    "# the nltk classifier expects a dictionary of features as input where the key is the feature name\n",
    "# and the value is the feature value\n",
    "\n",
    "# need to return a dict to work with the NLTK classifier\n",
    "# Possible problem for students: evaluate the difference \n",
    "# between using binarized features and using counts (non binarized features)\n",
    "def word_feats(doc_words: list, vocab: list, binary: bool = False, verbose: bool = False) -> dict:   \n",
    "    \"\"\"\n",
    "    Create BoW representations of the given data to be used by NLTK's NaiveBayesClassifier.\n",
    "    Args:\n",
    "        doc_words: list of words from a tokenized data sample \n",
    "        vocab: a list of words in the vocabulary\n",
    "        binary: whether or not to use binary features\n",
    "        verbose: whether or not to print additional information about the data being processed \n",
    "    Returns:\n",
    "        a dictionary representing the words present in both the vocab and doc_words \n",
    "        - for binary representations, in the format {word1: True, word2: True, ...}\n",
    "        - for multinomial representations, in the format {word1: count1, word2: count2, ...}\n",
    "    \"\"\" \n",
    "    # STUDENTS IMPLEMENT\n",
    "    doc_counter = Counter(doc_words)\n",
    "\n",
    "    # for efficiency, only iterate through words that we know are both in the vocab and in doc_words \n",
    "    overlap = set.intersection(set(doc_counter.keys()), set(vocab))\n",
    "\n",
    "    # initialize empty dictionary of features \n",
    "    bow_feats = {}\n",
    "    for word in overlap:\n",
    "        if binary:\n",
    "            bow_feats[word] = True\n",
    "        else:\n",
    "            bow_feats[word] = doc_counter[word]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Size of doc_words data:\", len(doc_words))\n",
    "        print(\"Size of vocab:\", len(vocab))\n",
    "        print(\"Size of overlap between doc_words and vocab:\", len(overlap))\n",
    "        print(\"Feature examples:\", list(bow_feats.items())[:3])\n",
    "        print()\n",
    "           \n",
    "    return bow_feats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of doc_words data: 138\n",
      "Size of vocab: 30705\n",
      "Size of overlap between doc_words and vocab: 87\n",
      "Feature examples: [('!', True), ('propaganda', True), ('value', True)]\n",
      "\n",
      "Binary:\n",
      "Predicted: 0  True: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set up & train a sentiment classifier using NLTK's NaiveBayesClassifier and\n",
    "# classify the first example in the dev set as an example\n",
    "# make sure your output is well-labeled\n",
    "# Should take < 10 sec to train (on Felix's computer this takes 0.5 sec)\n",
    "\n",
    "# test to make sure that you can train the classifier and use it to classify a new example\n",
    "vocab = sutils.create_index(X_train)\n",
    "\n",
    "# Training a classifier using binary features and predicting the label of the first dev example \n",
    "binary_train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    binary_train_data.append((word_feats(X_train[i], vocab, binary=True), y_train[i]))\n",
    "\n",
    "binary_classifier = NaiveBayesClassifier.train(binary_train_data)\n",
    "\n",
    "binary_dev_data = word_feats(X_dev[0], vocab, binary=True, verbose=True)\n",
    "prediction = binary_classifier.classify(binary_dev_data)\n",
    "print(\"Binary:\")\n",
    "print(\"Predicted:\", prediction, \" True:\", y_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of doc_words data: 138\n",
      "Size of vocab: 30705\n",
      "Size of overlap between doc_words and vocab: 87\n",
      "Feature examples: [('!', 1), ('propaganda', 3), ('value', 1)]\n",
      "\n",
      "Multinomial:\n",
      "Predicted: 0  True: 0\n"
     ]
    }
   ],
   "source": [
    "# Training a classifier using multinomial features and predicting the label of the first dev example \n",
    "multi_train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    multi_train_data.append((word_feats(X_train[i], vocab, binary=False), y_train[i]))\n",
    "\n",
    "multi_classifier = NaiveBayesClassifier.train(multi_train_data)\n",
    "\n",
    "multi_dev_data = word_feats(X_dev[0], vocab, binary=False, verbose=True)\n",
    "prediction = multi_classifier.classify(multi_dev_data)\n",
    "print(\"Multinomial:\")\n",
    "print(\"Predicted:\", prediction, \" True:\", y_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the provided dev set, evaluate your model with precision, recall, and f1 score as well as accuracy\n",
    "# You may use nltk's implemented `precision`, `recall`, `f_measure`, and `accuracy` functions\n",
    "# (make sure to look at the documentation for these functions!)\n",
    "# you will be creating a similar graph for logistic regression and neural nets, so make sure\n",
    "# you use functions wisely so that you do not have excessive repeated code\n",
    "# write any helper functions you need in sentiment_utils.py (functions that you'll use in your other notebooks as well)\n",
    "\n",
    "\n",
    "# create a graph of your classifier's performance on the dev set as a function of the amount of training data\n",
    "# the x-axis should be the amount of training data (as a percentage of the total training data)\n",
    "# the y-axis should be the performance of the classifier on the dev set\n",
    "# the graph should have 4 lines, one for each of precision, recall, f1, and accuracy\n",
    "# the graph should have a legend, title, and axis labels\n",
    "\n",
    "# takes approximately 3.5sec to run on Felix's computer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your model using both a __binarized__ and a __multinomial__ BoW. Use whichever one gives you a better final f1 score on the dev set to produce your graphs.\n",
    "\n",
    "- f1 score binarized: __YOUR ANSWER HERE__\n",
    "- f1 score multinomial: __YOUR ANSWER HERE__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
