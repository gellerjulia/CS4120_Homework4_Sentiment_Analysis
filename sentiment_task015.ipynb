{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 4: Sentiment Analysis - Task 0, Task 1, Task 5 (all primarily written tasks)\n",
    "----\n",
    "\n",
    "The following instructions are only written in this notebook but apply to all notebooks and `.py` files you submit for this homework.\n",
    "\n",
    "Due date: October 25th, 2023\n",
    "\n",
    "Points: \n",
    "- Task 0: 5 points\n",
    "- Task 1: 10 points\n",
    "- Task 2: 30 points\n",
    "- Task 3: 20 points\n",
    "- Task 4: 20 points\n",
    "- Task 5: 15 points\n",
    "\n",
    "Goals:\n",
    "- understand the difficulties of counting and probablities in NLP applications\n",
    "- work with real world data to build a functioning language model\n",
    "- stress test your model (to some extent)\n",
    "\n",
    "Complete in groups of: __two (pairs)__. If you prefer to work on your own, you may, but be aware that this homework has been designed as a partner project.\n",
    "\n",
    "Allowed python modules:\n",
    "- `numpy`, `matplotlib`, `keras`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, and all built-in python libraries (e.g. `math` and `string`)\n",
    "- if you would like to use a library not on this list, post on piazza to request permission\n",
    "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook. \n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should __and__ that all partners are included (for partner work).\n",
    "\n",
    "6120 students: complete __all__ problems.\n",
    "\n",
    "4120 students: you are not required to complete problems marked \"CS 6120 REQUIRED\". If you complete these you will not get extra credit. We will not take points off if you attempt these problems and do not succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names:  Julia Geller (4120) and Shae Marks (4120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 0: Name, References, Reflection (5 points)\n",
    "---\n",
    "\n",
    "References\n",
    "---\n",
    "List the resources you consulted to complete this homework here. Write one sentence per resource about what it provided to you. If you consulted no references to complete your assignment, write a brief sentence stating that this is the case and why it was the case for you.\n",
    "\n",
    "(Example)\n",
    "- https://docs.python.org/3/tutorial/datastructures.html\n",
    "\n",
    "    Read about the the basics and syntax for data structures in python.\n",
    "\n",
    "- https://www.nltk.org/_modules/nltk/classify/naivebayes.html\n",
    "\n",
    "     Read the documentation for NLTK to understand the functions and expected data for the Naive Bayes Classifier\n",
    "\n",
    "\n",
    "- https://stackoverflow.com/questions/9012487/savefig-outputs-blank-image\n",
    "\n",
    "    Debugging why our saved graphs were blank (needed to save before plt.show())\n",
    "    \n",
    "\n",
    "- https://www.educative.io/answers/keras-dropout-layer-implement-regularization\n",
    "\n",
    "    read about how Keras' Dropout layer works \n",
    "\n",
    "AI Collaboration\n",
    "---\n",
    "Following the *AI Collaboration Policy* in the syllabus, please cite any LLMs that you used here and briefly describe what you used them for. Additionally, provide comments in-line identifying the specific sections that you used LLMs on, if you used them towards the generation of any of your answers.\n",
    "\n",
    "__NEW__: Do not include nested list comprehensions supplied by AI collaborators â€” all nested lists comprehensions __must__ be re-written.\n",
    "\n",
    "Reflection\n",
    "----\n",
    "Answer the following questions __after__ you complete this assignment (no more than 1 sentence per question required, this section is graded on completion):\n",
    "\n",
    "1. Does this work reflect your best effort? \n",
    "\n",
    "Yes, this work reflects my best effort.\n",
    "\n",
    "2. What was/were the most challenging part(s) of the assignment?\n",
    "\n",
    "Training the neural network was a challenging part of this assignment because it took a long time and required optimization in order to be done in a reasonable timeframe. It was also challenging to conceptualize the difference featurized data structure between Naive Bayes and Logistic Regression/Neural Nets to ensure our abstracted functions handled all cases gracefully.\n",
    "\n",
    "3. If you want feedback, what function(s) or problem(s) would you like feedback on and why?\n",
    "\n",
    "We would like feedback on any questions we don't receive full credit so we can learn from our mistakes for the future.\n",
    "\n",
    "4. Briefly reflect on how your partnership functioned--who did which tasks, how was the workload on each of you individually as compared to the previous homeworks, etc.\n",
    "\n",
    "Julia Geller was responsible for task 3 and 4 while Shae Marks was responsible for task 0, 2, and 5. Julia implemented most general functions in sentiment_utils while we each implemented functions specific to our tasks. We both looked over eachother's work. We feel we both did an equal amount of work and found this homework more manageable because of our shared workload and consistent communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Provided Data Write-Up (10 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the __provided__ movie review data set.\n",
    "\n",
    "1. Where did you get the data from?\n",
    "\n",
    " The provided dataset(s) were sub-sampled from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews \n",
    "\n",
    "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)?\n",
    "\n",
    "The data is a collection of movie reviews from IMDB and were likely scraped from the site.\n",
    "\n",
    "3. (2 pts) How large is the dataset (answer for both the train and the dev set, separately)? (# reviews, # tokens in both the train and dev sets)\n",
    "\n",
    "The train set is made up of 1600 reviews with a total of 425421 tokens in those reviews (Using the  provided generate_tuples_from_file method). The dev set is made up of 200 reviews with a total of 54603 tokens in those reviews.\n",
    "\n",
    "\n",
    "4. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc)\n",
    "\n",
    "The data is highly polarized (distinctly negative or positive) movie reviews.\n",
    "\n",
    "5. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people)\n",
    "\n",
    "The authors of the reviews are a collection of IMDB users who have watched various movies. \n",
    "\n",
    "The dataset was published on Kaggle by user LAKSHMIPATHI N. The description on Kaggle links to a web page telling users to contact Andrew Maas for questions about this dataset. The web page belongs to Andrew Maas' Stanford website, so Andrew is likely a collector of this IMDB data.\n",
    "\n",
    "6. (2 pts) What is the distribution of labels in the data (answer for both the train and the dev set, separately)?\n",
    "\n",
    "The classes in the data are 0 and 1 (negative and positive, respectively). The training set has 796 reviews in class 0 and 804 reviews in class 1. The dev set has 95 reviews in class 0 and 105 reviews in class 1.\n",
    "\n",
    "\n",
    "7. (2 pts) How large is the vocabulary (answer for both the train and the dev set, separately)?\n",
    "\n",
    "\n",
    "We used the provided generate_tuples_from_file function to tokenize the reviews and took the set of those tokens to get the vocab size. The training set has a vocab size of 30705, and the dev set has a vocab size of 8953.\n",
    "\n",
    "\n",
    "8. (1 pt) How big is the overlap between the vocabulary for the train and dev set?\n",
    "\n",
    "The train and dev vocabs have an overlap of 6574 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# our utility functions\n",
    "# RESTART your jupyter notebook kernel if you make changes to this file\n",
    "import sentiment_utils as sutils\n",
    "import math\n",
    "import statistics as stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train example: ['Van', 'Dien', 'must', 'cringe', 'with', 'embarrassment', 'at', 'the', 'memory', 'of', 'this', 'ludicrously', 'poor', 'film', ',', 'as', 'indeed', 'must', 'every', 'single', 'individual', 'involved', '.', 'To', 'be', 'honest', 'I', 'am', 'rather', 'embarrassed', 'to', 'admit', 'I', 'watched', 'it', 'from', 'start', 'to', 'finish', '.', 'Production', 'values', 'are', 'somewhere', 'between', 'the', 'original', 'series', 'of', \"'Crossroads\", \"'\", 'and', \"'Prisoner\", 'Cell', 'Block', 'H', \"'\", '.', 'Most', 'five', 'year', 'olds', 'would', 'be', 'able', 'to', 'come', 'up', 'with', 'more', 'realistic', 'dialogue', 'and', 'a', 'more', 'plausible', 'plot', '.', 'As', 'for', 'the', 'acting', 'performances', ',', 'if', 'you', 'can', 'imagine', 'the', 'most', 'rubbish', 'porno', 'you', 'have', 'ever', 'seen', '-', 'one', 'of', 'those', 'ones', 'where', 'the', 'action', 'is', 'padded', 'out', 'with', 'some', 'interminable', \"'story\", \"'\", 'to', 'explain', 'how', 'some', 'pouting', 'old', 'peroxide', 'blonde', 'boiler', 'has', 'come', 'to', 'be', 'getting', 'spit-roasted', 'by', 'a', 'couple', 'of', 'blokes', 'with', 'moustaches', '-', 'you', 'will', 'have', 'some', 'idea', 'of', 'the', 'standard', 'of', 'acting', 'in', \"'Maiden\", 'Voyage', \"'\", '.', 'Worse', 'still', ',', 'you', 'ca', \"n't\", 'even', 'fast', 'forward', 'to', 'the', 'sex', 'scenes', ',', 'because', 'there', 'are', \"n't\", 'any', '.', 'An', 'appallingly', 'dreadful', 'film', '.']\n",
      "y_train example: 0\n",
      "Number of reviews in the training set: 1600\n",
      "Number of tokens in training set: 425421 \n",
      "\n",
      "\n",
      "X_dev example: ['The', 'movie', \"'Gung\", 'Ho', '!', \"'\", ':', 'The', 'Story', 'of', 'Carlson', \"'s\", 'Makin', 'Island', 'Raiders', 'was', 'made', 'in', '1943', 'with', 'a', 'view', 'to', 'go', 'up', 'the', 'moral', 'of', 'American', 'people', 'at', 'the', 'duration', 'of', 'second', 'world', 'war', '.', 'It', 'shows', 'with', 'the', 'better', 'way', 'that', 'the', 'cinema', 'can', 'constitute', 'body', 'of', 'propaganda', '.', 'The', 'value', 'of', 'this', 'film', 'is', 'only', 'collection', 'and', 'no', 'artistic', '.', 'In', 'a', 'film', 'of', 'propaganda', 'it', 'is', 'useless', 'to', 'judge', 'direction', 'and', 'actors', '.', 'Watch', 'that', 'movie', 'if', 'you', 'are', 'interested', 'to', 'learn', 'how', 'propaganda', 'functions', 'in', 'the', 'movies', 'or', 'if', 'you', 'are', 'a', 'big', 'fun', 'of', 'Robert', 'Mitchum', 'who', 'has', 'a', 'small', 'role', 'in', 'the', 'film', '.', 'If', 'you', 'want', 'to', 'see', 'a', 'film', 'for', 'the', 'second', 'world', 'war', ',', 'they', 'exist', 'much', 'better', 'and', 'objective', '.', 'I', 'rated', 'it', '4/10', '.']\n",
      "y_dev example: 0\n",
      "Number of reviews in the dev set: 200\n",
      "Number of tokens in dev set: 54603 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feel free to write code to help answer the above questions\n",
    "from collections import Counter\n",
    "\n",
    "# DETERMINING SIZE OF THE DATASET \n",
    "TRAIN_FILE = \"movie_reviews_train.txt\"\n",
    "DEV_FILE = \"movie_reviews_dev.txt\"\n",
    "\n",
    "train_tups = sutils.generate_tuples_from_file(TRAIN_FILE)\n",
    "dev_tups = sutils.generate_tuples_from_file(DEV_FILE)\n",
    "\n",
    "X_train, y_train = train_tups\n",
    "X_dev, y_dev = dev_tups\n",
    "\n",
    "# TRAIN \n",
    "print(\"X_train example:\", X_train[0])\n",
    "print(\"y_train example:\", y_train[0])\n",
    "print('Number of reviews in the training set:', len(X_train))\n",
    "assert len(X_train) == len(y_train)\n",
    "\n",
    "# flatten list of list \n",
    "train_tokens = list(sum(X_train,[]))\n",
    "print('Number of tokens in training set:', len(train_tokens), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# DEV\n",
    "print(\"X_dev example:\", X_dev[0])\n",
    "print(\"y_dev example:\", y_dev[0])\n",
    "print('Number of reviews in the dev set:', len(X_dev))\n",
    "assert len(X_dev) == len(y_dev)\n",
    "\n",
    "# flatten list of list \n",
    "dev_tokens = list(sum(X_dev,[]))\n",
    "print('Number of tokens in dev set:', len(dev_tokens), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label values in training data: {0, 1}\n",
      "Number of reviews with 0 class in training set: 796\n",
      "Number of reviews with 1 class in training set: 804\n",
      "\n",
      "Unique label values in dev data: {0, 1}\n",
      "Number of reviews with 0 class in dev set: 95\n",
      "Number of reviews with 1 class in dev set: 105\n"
     ]
    }
   ],
   "source": [
    "# DETERMINING DISTRIBUTION OF LABELS \n",
    "\n",
    "# TRAIN\n",
    "print('Unique label values in training data:', set(y_train))\n",
    "train_label_count = Counter(y_train)\n",
    "print('Number of reviews with 0 class in training set:', train_label_count[0])\n",
    "print('Number of reviews with 1 class in training set:', train_label_count[1])\n",
    "assert train_label_count[0] +  train_label_count[1] == 1600\n",
    "print()\n",
    "\n",
    "# DEV\n",
    "print('Unique label values in dev data:', set(y_dev))\n",
    "dev_label_count = Counter(y_dev)\n",
    "print('Number of reviews with 0 class in dev set:', dev_label_count[0])\n",
    "print('Number of reviews with 1 class in dev set:', dev_label_count[1])\n",
    "assert dev_label_count[0] +  dev_label_count[1] == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training vocab: 30705\n",
      "Size of dev vocab: 8953\n",
      "Size of vocab overlap: 6574\n"
     ]
    }
   ],
   "source": [
    "# DETERMINING SIZE OF VOCAB \n",
    "\n",
    "# TRAIN\n",
    "train_vocab = set(train_tokens)\n",
    "print(\"Size of training vocab:\", len(train_vocab))\n",
    "\n",
    "\n",
    "# DEV \n",
    "dev_vocab = set(dev_tokens)\n",
    "print(\"Size of dev vocab:\", len(dev_vocab))\n",
    "\n",
    "# OVERLAP\n",
    "print(\"Size of vocab overlap:\", len(set.intersection(train_vocab, dev_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Model Evaluation (15 points)\n",
    "---\n",
    "Save your three graph files for the __best__ configurations that you found with your models using the `plt.savefig(filename)` command. The `bbox_inches` optional parameter will help you control how much whitespace outside of the graph is in your resulting image.\n",
    "Run your each notebook containing a classifier 3 times, resulting in __NINE__ saved graphed (don't just overwrite your previous ones).\n",
    "\n",
    "You will turn in all of these files.\n",
    "\n",
    "10 points in this section are allocated for having all nine graphs legible, properly labeled, and present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. (1 pt) When using __10%__ of your data, which model had the highest f1 score?\n",
    "\n",
    "The Logistic Regression  model had the highest average f1 score (0.725) when trained on 10% of the data and tested on the full dev set. \n",
    "\n",
    "2. (1 pt) Which classifier had the most __consistent__ performance (that is, which classifier had the least variation across all three graphs you have for it -- no need to mathematically calculate this, you can just look at the graphs)? \n",
    "\n",
    "The 3 Neural Network graphs appear the most similar to each other, so this model performed the most consistently. Logistic Regression was also fairly consistent.\n",
    "\n",
    "3. (1 pt) For each model, what percentage of training data resulted in the highest f1 score?\n",
    "    1. Naive Bayes: When 50% of the training data was used \n",
    "    2. Logistic Regression: When 90-100% of the training data was used \n",
    "    3. Neural Net: When 100% of the training data was used \n",
    "\n",
    "I determined this by observing where the f1 score tended to peak across the graphs.\n",
    "\n",
    "4. (2 pts) Which model, if any, appeared to overfit the training data the most? Why?\n",
    "\n",
    "The Logistic Regression model seems the most overfit to the training data because it performs perfectly when trained and tested on the same data but then experiences a larger drop-off in performance when tested on the dev data. (Accuracy 1.0 to accuracy 0.805). This model may be better able to generalize to new data if it included a regularization method to prevent it from fitting so closely to the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get featurized data\n",
    "\n",
    "# NAIVE BAYES - use binary because it had the best f1 score\n",
    "vocab = train_vocab\n",
    "X_train_feats_nb = sutils.naive_bayes_featurize(X_train, vocab, binary=True)\n",
    "nb_train_data = [(x, y) for x,y in zip(X_train_feats_nb, y_train)]\n",
    "\n",
    "X_dev_feats_nb = sutils.naive_bayes_featurize(X_dev, vocab, binary=True)\n",
    "nb_dev_data = [(x, y) for x,y in zip(X_dev_feats_nb, y_dev)]\n",
    "\n",
    "# LOGISTIC REGRESSION  - use binary with CountVectorizer because it had the best f1 score\n",
    "X_train_flat = [' '.join(row) for row in X_train]\n",
    "X_dev_flat = [' '.join(row) for row in X_dev]\n",
    "\n",
    "X_train_CV_binary, X_dev_CV_binary = sutils.featurize('CV', X_train_flat, X_dev_flat, binary = True)\n",
    "\n",
    "log_reg_train_feats = [(x, y) for x,y in zip(X_train_CV_binary, y_train)]\n",
    "log_reg_dev_feats = [(x, y) for x,y in zip(X_dev_CV_binary, y_dev)]\n",
    "\n",
    "# NEURAL NETWORK - use binary with CountVectorizer because it had the best f1 score\n",
    "nn_train_feats = [(x, y) for x,y in zip(X_train_CV_binary, y_train)]\n",
    "nn_dev_feats = [(x, y) for x,y in zip(X_dev_CV_binary, y_dev)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes f1 score when trained on 10 percent of data: 0.590088693907497\n",
      "Logistic Regression f1 score when trained on 10 percent of data: 0.7256515845342939\n",
      "Neural Network f1 score when trained on 10 percent of data: 0.5705984415966634\n"
     ]
    }
   ],
   "source": [
    "# Finding F1 scores with only 10% of data \n",
    "\n",
    "# NAIVE BAYES\n",
    "nb_f1_score_samples = []\n",
    "for i in range(20):\n",
    "    random.shuffle(nb_train_data)\n",
    "    nb_train_data_10 = nb_train_data[0:math.ceil(len(nb_train_data)*.1)] # get 10% of training data \n",
    "    X_train_nb_10 = [tup[0] for tup in nb_train_data_10]\n",
    "    y_train_nb_10 = [tup[1] for tup in nb_train_data_10]\n",
    "\n",
    "    f1_nb_10 = sutils.naive_bayes_metrics(X_train_nb_10, y_train_nb_10, X_dev_feats_nb, y_dev)[2] # get f1 score \n",
    "    nb_f1_score_samples.append(f1_nb_10)\n",
    "\n",
    "f1_nb_10 = stats.mean(nb_f1_score_samples)\n",
    "\n",
    "\n",
    "# LOGISTIC REGRESSION \n",
    "log_reg_f1_score_samples = []\n",
    "for i in range(20):\n",
    "    random.shuffle(log_reg_train_feats)\n",
    "\n",
    "    log_reg_train_10 = log_reg_train_feats[0:math.ceil(len(log_reg_train_feats)*.1)]\n",
    "    X_train_log_reg_10 = [tup[0] for tup in log_reg_train_10]\n",
    "    y_train_log_reg_10 = [tup[1] for tup in log_reg_train_10]\n",
    "\n",
    "    f1_log_reg_10 = sutils.log_reg_metrics(X_train_log_reg_10, y_train_log_reg_10, X_dev_CV_binary, y_dev)[2]\n",
    "    log_reg_f1_score_samples.append(f1_log_reg_10)\n",
    "\n",
    "f1_log_reg_10 = stats.mean(log_reg_f1_score_samples)\n",
    "\n",
    "\n",
    "# NEURAL NETWORKS \n",
    "nn_f1_score_samples = []\n",
    "for i in range(20):\n",
    "    random.shuffle(nn_train_feats)\n",
    "\n",
    "    nn_train_10 = nn_train_feats[0:math.ceil(len(nn_train_feats)*.1)]\n",
    "    X_train_nn_10 = [tup[0] for tup in nn_train_10]\n",
    "    y_train_nn_10 = [tup[1] for tup in nn_train_10]\n",
    "\n",
    "    f1_nn_10 = sutils.neural_net_metrics(X_train_nn_10, y_train_nn_10, X_dev_CV_binary, y_dev, num_epochs=10, neural_net_verbose=False)[2]\n",
    "    nn_f1_score_samples.append(f1_nn_10)\n",
    "\n",
    "f1_nn_10 = stats.mean(nn_f1_score_samples)\n",
    "\n",
    "print()\n",
    "print(\"Naive Bayes f1 score when trained on 10 percent of data:\", f1_nb_10)\n",
    "print(\"Logistic Regression f1 score when trained on 10 percent of data:\", f1_log_reg_10)\n",
    "print(\"Neural Network f1 score when trained on 10 percent of data:\", f1_nn_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes: How well does it perform when trained and evaluated on the same data?\n",
      "Precision: 0.9987405541561712  Recall: 0.986318407960199  f1: 0.9924906132665833  Accuracy: 0.9925\n",
      "\n",
      "Naive Bayes: How well does it perform when trained and evaluated on different data?\n",
      "Precision: 0.8681318681318682  Recall: 0.7523809523809524  f1: 0.8061224489795917  Accuracy: 0.81\n",
      "\n",
      "Logistic Regression: How well does it perform when trained and evaluated on the same data?\n",
      "Precision: 1.0  Recall: 1.0  f1: 1.0  Accuracy: 1.0\n",
      "\n",
      "Logistic Regression: How well does it perform when trained and evaluated on different data?\n",
      "Precision: 0.7946428571428571  Recall: 0.8476190476190476  f1: 0.8202764976958524  Accuracy: 0.805\n",
      "\n",
      "Neural Network: How well does it perform when trained and evaluated on the same data?\n",
      "Precision: 0.9058240396530359  Recall: 0.9092039800995025  f1: 0.9075108628181254  Accuracy: 0.906875\n",
      "\n",
      "Neural Network: How well does it perform when trained and evaluated on different data?\n",
      "Precision: 0.7642276422764228  Recall: 0.8952380952380953  f1: 0.824561403508772  Accuracy: 0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DETERMINE OVERFITTING\n",
    "# Overfitting is when the model performs well on the training data but poorly on the dev data because it is not generalizable \n",
    "# Let's look at the scores for each model on the training vs on the dev data \n",
    "\n",
    "print(\"Naive Bayes: How well does it perform when trained and evaluated on the same data?\")\n",
    "sutils.naive_bayes_metrics(X_train_feats_nb, y_train, X_train_feats_nb, y_train, verbose=True)\n",
    "print(\"\\nNaive Bayes: How well does it perform when trained and evaluated on different data?\")\n",
    "sutils.naive_bayes_metrics(X_train_feats_nb, y_train, X_dev_feats_nb, y_dev, verbose=True)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Logistic Regression: How well does it perform when trained and evaluated on the same data?\")\n",
    "sutils.log_reg_metrics(X_train_CV_binary, y_train, X_train_CV_binary, y_train, verbose=True)\n",
    "print(\"\\nLogistic Regression: How well does it perform when trained and evaluated on different data?\")\n",
    "sutils.log_reg_metrics(X_train_CV_binary, y_train, X_dev_CV_binary, y_dev, verbose=True)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Neural Network: How well does it perform when trained and evaluated on the same data?\")\n",
    "sutils.neural_net_metrics(X_train_CV_binary, y_train, X_train_CV_binary, y_train, num_epochs=10, verbose=True, neural_net_verbose=False)\n",
    "print(\"\\nNeural Network: How well does it perform when trained and evaluated on different data?\")\n",
    "sutils.neural_net_metrics(X_train_CV_binary, y_train, X_dev_CV_binary, y_dev, num_epochs=10, verbose=True, neural_net_verbose=False)\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6120 REQUIRED\n",
    "----\n",
    "\n",
    "Find a second data set that is labeled for sentiment from a different domain (not movie reivews). Rerun your notebook with this data (you should set up your notebook so that you only need to change the paths and possibly run a different pre-processing function on the data). Note that you will want binary labels.\n",
    "\n",
    "Answer the regular data questions for your new data set\n",
    "----\n",
    "1. Where did you get the data from?\n",
    "2. How was the data collected (where did the people acquiring the data get it from and how)?\n",
    "3. How large is the dataset (answer for both the train and the dev set, separately)? (# reviews, # tokens in both the train and dev sets)\n",
    "4. What is your data? (i.e. newswire, tweets, books, blogs, etc)\n",
    "5. Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people)\n",
    "6. What is the distribution of labels in the data (answer for both the train and the dev set, separately)?\n",
    "7. How large is the vocabulary (answer for both the train and the dev set, separately)?\n",
    "8. How big is the overlap between the vocabulary for the train and dev set?\n",
    "\n",
    "Answer the model evaluation questions for your new data set\n",
    "----\n",
    "1. When using __10%__ of your data, which model had the highest f1 score?\n",
    "2. Which classifier had the most __consistent__ performance (that is, which classifier had the least variation across all three graphs you have for it -- no need to mathematically calculate this, you can just look at the graphs)?\n",
    "3. For each model, what percentage of training data resulted in the highest f1 score?\n",
    "    1. Naive Bayes:\n",
    "    2. Logistic Regression:\n",
    "    3. Neural Net:\n",
    "4. Which model, if any, appeared to overfit the training data the most? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any code you need to write here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
